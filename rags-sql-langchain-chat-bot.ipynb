{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iggyloh/Tesla-and-amazon-stock-dashboard/blob/main/DAC_proj_2_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYr8f8iV92jF",
        "outputId": "d290e5eb-0911-4030-f4b4-f737022cda2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\script.pc #93\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import  display\n",
        "import pandas as pd\n",
        "from typing import TypedDict, Annotated\n",
        "import sqlite3\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from langchain.text_splitter  import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph.state import StateGraph, START, END\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "from langchain.schema import BaseMessage\n",
        "from langchain_community.vectorstores import Chroma, InMemoryVectorStore\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain.schema.document import Document\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Loading necessary APIs and files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Loading APIs and connecting embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA0Cd2xP9tb-",
        "outputId": "07129175-d7c0-4721-a48f-62159a3161f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\script.pc #93\\AppData\\Local\\Temp\\ipykernel_17212\\2742753873.py:16: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding_model = OpenAIEmbeddings(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lee \"Faker\" Sang-hyeok from South Korea has won the most League of Legends World Championships, with a total of three titles. He won the championships in 2013, 2015, and 2016 with his team, SK Telecom T1.\n"
          ]
        }
      ],
      "source": [
        "# Load API env\n",
        "load_dotenv('api.env')\n",
        "\n",
        "# Langsmith tracing\n",
        "trace = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "\n",
        "# OpenAi model\n",
        "gpt = ChatOpenAI(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    temperature = 0.1)\n",
        "\n",
        "\n",
        "# Embedding model\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    model = 'text-embedding-ada-002',\n",
        "    openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        ")\n",
        "\n",
        "\n",
        "# Small test\n",
        "try:\n",
        "    response = gpt.invoke('Which player won the most league of legends world championships?')\n",
        "    print(response.content)\n",
        "except Exception as e:\n",
        "    print(f\"Error {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2. Loading and cleaning papers.csv\n",
        "We collect only the column 'full_text' and turn it into a pandas series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- This is the raw CSV: ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>full_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27</td>\n",
              "      <td>1987</td>\n",
              "      <td>Bit-Serial Neural Networks</td>\n",
              "      <td>NaN</td>\n",
              "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63</td>\n",
              "      <td>1987</td>\n",
              "      <td>Connectivity Versus Entropy</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>1987</td>\n",
              "      <td>The Hopfield Model with Multi-Level Neurons</td>\n",
              "      <td>NaN</td>\n",
              "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>1987</td>\n",
              "      <td>How Neural Nets Work</td>\n",
              "      <td>NaN</td>\n",
              "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>69</td>\n",
              "      <td>1987</td>\n",
              "      <td>Spatial Organization of Neural Networks: A Pro...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   source_id  year                                              title  \\\n",
              "0         27  1987                         Bit-Serial Neural Networks   \n",
              "1         63  1987                        Connectivity Versus Entropy   \n",
              "2         60  1987        The Hopfield Model with Multi-Level Neurons   \n",
              "3         59  1987                               How Neural Nets Work   \n",
              "4         69  1987  Spatial Organization of Neural Networks: A Pro...   \n",
              "\n",
              "  abstract                                          full_text  \n",
              "0      NaN  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...  \n",
              "1      NaN  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...  \n",
              "2      NaN  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...  \n",
              "3      NaN  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...  \n",
              "4      NaN  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- This is the pandas series of ['full_texts'] randomly sampling 10 papers: ---\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "8391    Robust Attribution Regularization\\n\\nJiefeng C...\n",
              "3068    Information Bottleneck Optimization and\\n\\nInd...\n",
              "7231    EEG-GRAPH: A Factor-Graph-Based Model for\\nCap...\n",
              "1965    Spectral Kernel Methods for Clustering \\n\\nN e...\n",
              "4704    A Conditional Multinomial Mixture Model for\\n\\...\n",
              "608     Information,  prediction,  and  query by \\n\\nc...\n",
              "5294    Local Decorrelation for Improved Pedestrian De...\n",
              "9254    Identiﬁcation of Conditional Causal Effects\\n\\...\n",
              "9285    Beyond Conﬁdence Regions: Tight Bayesian\\n\\nAm...\n",
              "8434    Chirality Nets for Human Pose Regression\\n\\nRa...\n",
              "Name: full_text, dtype: object"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Print and display csv\n",
        "df = pd.read_csv('papers.csv')\n",
        "print(\"--- This is the raw CSV: ---\")\n",
        "display(df.head())\n",
        "\n",
        "\n",
        "# Due to the pressence of 3 NA entries, we fill them with \" \"\n",
        "texts = df['full_text'].fillna(\" \")\n",
        "\n",
        "\n",
        "# To save on cost and use this as a POC/MVP we only keep 10 entries (randomly samplied)\n",
        "texts = texts.sample(n=10, random_state=69)\n",
        "print(\"--- This is the pandas series of ['full_texts'] randomly sampling 10 papers: ---\")\n",
        "display(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3. Defining Parent Graph before RAGS and SQL Graph and memory config for persistence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parent graph\n",
        "class ParentGraph(TypedDict):\n",
        "    \"\"\"Represents the state of our graph. Enforcing questions and answers to be dtype string. -spnc\"\"\"\n",
        "    question: str\n",
        "    question_type: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Config for memory saver, only need 1 thread here\n",
        "# We're defining this early to check if the sql builder works\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        'thread_id': '1',                  \n",
        "        'checkpoint_ns': 'checkpoint',      \n",
        "        'checkpoint_id': '13376969god'       \n",
        "    }\n",
        "}\n",
        "\n",
        "memory = MemorySaver() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Building the RAGS and RAGS Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1. Embedding each research paper and storing in a vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQmGr8gVpA3V",
        "outputId": "f0a47824-9ef8-4a03-80c7-af6c7f75d2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 3 document IDs: ['1a2ea0be-1dd4-49f6-965d-48ae33f6395e', '244feca5-2962-47b8-a5c9-232a2dbec1b1', 'e0055cfc-b309-441f-8600-99f060c0c344']\n",
            "First 3 document contents: ['Robust Attribution Regularization\\n\\nJiefeng Chen ⇤ 1 Xi Wu ⇤ 2 Vaibhav Rastogi †2\\n\\n1 University of Wisconsin-Madison\\n\\nYingyu Liang 1\\n\\nSomesh Jha 1,3\\n\\n2 Google\\n\\n3 XaiPient\\n\\nAbstract', 'An emerging problem in trustworthy machine learning is to train models that pro-\\nduce robust interpretations for their predictions. We take a step towards solving\\nthis problem through the lens of axiomatic attribution of neural networks. Our\\ntheory is grounded in the recent work, Integrated Gradients (IG) [STY17], in\\naxiomatically attributing a neural network’s output change to its input change.\\nWe propose training objectives in classic robust optimization models to achieve\\nrobust IG attributions. Our objectives give principled generalizations of previous\\nobjectives designed for robust predictions, and they naturally degenerate to classic\\nsoft-margin training for one-layer neural networks. We also generalize previous\\ntheory and prove that the objectives for different robust optimization models are\\nclosely related. Experiments demonstrate the effectiveness of our method, and\\nalso point to intriguing problems which hint at the need for better optimization', 'closely related. Experiments demonstrate the effectiveness of our method, and\\nalso point to intriguing problems which hint at the need for better optimization\\ntechniques or better neural network architectures for robust attribution training.']\n",
            "Number of embeddings: 491\n",
            "Shape of each embedding (dimensions): 1536\n"
          ]
        }
      ],
      "source": [
        "# Object \"vector_store\" will hold each research paper and their corresponding embedded values\n",
        "vector_store = InMemoryVectorStore(embedding_model)\n",
        "\n",
        "\n",
        "# Turning pandas series into document objects\n",
        "documents = [Document(page_content=text) for text in texts]\n",
        "\n",
        "\n",
        "# Chunking texts into smaller sizes\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "all_splits = [Document(page_content=chunk) for doc in documents for chunk in text_splitter.split_text(doc.page_content)]\n",
        "\n",
        "\n",
        "# Generate embeddings for each chunk\n",
        "embeddings = embedding_model.embed_documents([doc.page_content for doc in all_splits])\n",
        "\n",
        "\n",
        "# Store and embed in vector store simultaneously\n",
        "document_ids = vector_store.add_documents( # Can be ran again later to add more research papers yeah\n",
        "    documents=all_splits,\n",
        "    embeddings=embeddings \n",
        ")\n",
        "\n",
        "\n",
        "# Verifying embedding done and document IDs\n",
        "print(f\"First 3 document IDs: {document_ids[:3]}\")\n",
        "print(f\"First 3 document contents: {[doc.page_content for doc in all_splits[:3]]}\")\n",
        "print(f\"Number of embeddings: {len(embeddings)}\")\n",
        "print(f\"Shape of each embedding (dimensions): {len(embeddings[0])}\") \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Testing retrieval from the vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backpropagation is a learning algorithm for continually running fully recurrent neural networks. This information is from the paper \"A learning algorithm for continually running fully recurrent neural networks\" by R. J. Williams and D. Zipser in Neural Computation, 1989.\n",
            "Retrieved documents: [Document(id='a949ef01-b618-4d37-9c75-519125bece58', metadata={}, page_content='in Proceedings of the 16th conference on Uncertainty in artiﬁcial intelligence, 2000, pp. 626–633.\\n\\n[15] J. S. Yedidia, W. T. Freeman, Y. Weiss et al., “Generalized belief propagation,” in Advances in Neural\\n\\nInformation Processing Systems, vol. 13, 2000, pp. 689–695.\\n\\n[16] W. R. Gilks, S. Richardson, and D. Spiegelhalter, Markov chain Monte Carlo in practice. CRC Press,\\n\\n1995.\\n\\n[17] S. Chib and E. Greenberg, “Understanding the Metropolis-Hastings algorithm,” The American Statisti-\\n\\ncian, vol. 49, no. 4, pp. 327–335, 1995.\\n\\n[18] V. Kolmogorov and R. Zabin, “What energy functions can be minimized via graph cuts?” IEEE Transac-\\n\\ntions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, pp. 147–159, 2004.\\n\\n[19] R. G. Andrzejak, D. Chicharro, C. E. Elger, and F. Mormann, “Seizure prediction: Any better than\\n\\nchance?” Clinical Neurophysiology, vol. 120, no. 8, pp. 1465–1478, 2009.'), Document(id='ac4a12f2-adfc-4806-87ac-d3fba7436c5e', metadata={}, page_content='on this potential connection seems warranted.\\nDifﬁculty of optimization. While our experimental results are encouraging, we observe that when\\ntraining stops, the attribution regularization term remains signiﬁcant (typically around tens to hun-\\ndreds), which indicates ineffective optimization for the objectives. To this end, a main problem is\\nnetwork depth, where as depth increases, we get very unstable trajectories of gradient descent, which\\nseems to be related to the use of second order information during robust attribution optimization (due\\nto summation approximation, we have ﬁrst order terms in the training objectives). Therefore, it is\\nnatural to further study better optimization tchniques or better architectures for robust attribution\\ntraining.'), Document(id='453a13e7-a6d0-46bc-9c87-aaab544ac476', metadata={}, page_content='prevent neural networks from overﬁtting. JMLR, 2014.\\n\\n[51] X. Sun, J. Shang, S. Liang, and Y. Wei. Compositional human pose regression. In Proc. ICCV, 2017.\\n\\n[52] B. Tekin, P. Márquez-Neila, M. Salzmann, and P. Fua. Learning to fuse 2D and 3D image cues for\\n\\nmonocular body pose estimation. In Proc. ICCV, 2017.\\n\\n[53] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri. A closer look at spatiotemporal\\n\\nconvolutions for action recognition. In Proc. CVPR, 2018.\\n\\n[54] M. Vetterli, J. Kovaˇcevi´c, and V. K. Goyal. Foundations of signal processing. Cambridge University Press,\\n\\n2014.\\n\\n[55] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay\\n\\nneural networks. Backpropagation: Theory, Architectures and Applications, 1995.\\n\\n[56] R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural networks.\\n\\nNeural computation, 1989.'), Document(id='9edacada-e277-4111-a61a-4ab28cbfa37b', metadata={}, page_content='Understanding  this  relationship  is  a  main goal of the  present  work,  and enables  us \\nto prove a positive result about the power of queries.  Our work is derived within the \\nquery filtering paradigm, rather than the  constructive paradigm.  In this  paradigm, \\nproposed  by  [CAL90],  the  learner  is  given  access  to  a  stream  of inputs  drawn  at \\nrandom from  a  distribution.  The learner  sees  every  input,  but  chooses  whether  or \\nnot  to query  the teacher  for  the  label.  This paradigm is  realistic in  contexts  where \\nit  is  cheap  to  get  unlabeled  examples,  but  expensive  to  label  them.  It avoids  the \\nproblems  with  the  constructive  paradigm described  in  [ER90]  because  it gives  the \\nlearner free  access  to the input distribution.')]\n"
          ]
        }
      ],
      "source": [
        "# Golden retriever from the vector store, who's a good boy?\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "\n",
        "# RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=gpt, \n",
        "    chain_type=\"stuff\",  # 'stuff' means the full document is fed to the model, not a restaurant lolol\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "\n",
        "# Example query, if the RAGS works, we should be quoting from one of the papers we fed it\n",
        "query = \"What is backpropogation? Quote the paper you are retrieving from.\"\n",
        "\n",
        "\n",
        "# Use the QA chain to answer the query based on retrieved documents\n",
        "response = qa_chain.run(query)\n",
        "print(response)\n",
        "\n",
        "\n",
        "# Retrieve documents manually, check if the mentioned paper is within the results (spoiler: yes it works)\n",
        "results = retriever.get_relevant_documents(\"What is backpropogation?\")\n",
        "print(f\"Retrieved documents: {results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. RAG SubGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class states that ensures questions and answers are string\n",
        "class RagState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph. Enforcing questions and answers to be dtype string. -spnc\"\"\"\n",
        "    question: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# RAG SubGraph to be called if it is a question answerable by RAG\n",
        "def rag_subgraph(state: ParentGraph):\n",
        "    \"\"\"Handle RAG-specific questions.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retieving done like in the above testing cell\n",
        "    search_results = retriever.get_relevant_documents(question)\n",
        "    \n",
        "    # Combine all retrieved documents for MAXIMUM CONTEXT\n",
        "    context = \"\\n\".join([doc.page_content for doc in search_results])\n",
        "    \n",
        "    response = qa_chain.run(question + \"\\n\" + context)  \n",
        "\n",
        "    # If we can't answer with the RAG then we won't have an answer I guess, ask a better question T_T\n",
        "    answer = response if response else \"No response available from training, ask a question relating to Deep Learning research in the past 30 years.\"\n",
        "    \n",
        "    return {\"answer\": answer}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Building the SQL Graph from 'main.db' generated by 'sql-data-exploration-database-creation.ipynb'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1. Defining our own prompt template because we are gangster like that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pulling prompt template form LangChain\n",
        "query_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"dialect\", \"top_k\", \"table_info\", \"input\"],\n",
        "    template=\"\"\"\n",
        "    You are a SQL query generator. Given the following information:\n",
        "    - Database dialect: {dialect}\n",
        "    - Tables: {table_info}\n",
        "    - Question: {input}\n",
        "\n",
        "    Generate the best SQL query to answer the question.\n",
        "    Provide only the query, and make sure it is syntactically valid.\n",
        "    The fields are full_name from combined_df if they mention about author names.\n",
        "\n",
        "    You can do your own descriptive statistics like average papers each year etc.\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. SQL SubGraph built on functions to execute syntatically correct queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class states that ensures questions and answers are string\n",
        "class SqlState(TypedDict):\n",
        "    \"\"\"Represents the state of our graph. Enforcing questions and answers to be dtype string. -spnc\"\"\"\n",
        "    question: str\n",
        "    query: str\n",
        "    result: str\n",
        "    answer: str\n",
        "\n",
        "\n",
        "# Class states to ensure query output is string and eventually syntatically correct\n",
        "class QueryOutput(TypedDict):\n",
        "    \"\"\"Generated SQL query.\"\"\"\n",
        "    query: Annotated[str, ..., \"Syntactically valid SQL query.\"]\n",
        "\n",
        "\n",
        "# Function to get ALL table infos \n",
        "def get_table_info():\n",
        "    \"\"\"Return information about the tables in the database.\"\"\"\n",
        "    conn = sqlite3.connect('main.db')\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Query to get all table names\n",
        "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "    tables = cursor.fetchall()\n",
        "    \n",
        "    # Optionally, get more detailed info if needed\n",
        "    conn.close()\n",
        "    \n",
        "    return tables\n",
        "\n",
        "\n",
        "# Function to write syntatically correct SQLite3 queries\n",
        "def write_query(state: SqlState):\n",
        "    \"\"\"Generate SQL query to fetch information.\"\"\"\n",
        "    \n",
        "    # Open the SQLite connection here\n",
        "    conn = sqlite3.connect('main.db')\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Your code to generate the SQL query based on the question\n",
        "    prompt = query_prompt_template.invoke(\n",
        "        {\n",
        "            \"dialect\": \"sqlite\",  \n",
        "            \"top_k\": 5,\n",
        "            \"table_info\": get_table_info(), \n",
        "            \"input\": state[\"question\"],\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    structured_llm = gpt.with_structured_output(QueryOutput)\n",
        "    result = structured_llm.invoke(prompt)\n",
        "    \n",
        "    # Optionally, you could run the query directly here if needed for debugging\n",
        "    generated_query = result[\"query\"]\n",
        "    \n",
        "    # Return the query string\n",
        "    conn.close() \n",
        "    return {\"query\": generated_query}\n",
        "\n",
        "\n",
        "# Function to execute the query generated from write_query function\n",
        "def execute_query(state: SqlState):\n",
        "    \"\"\"Execute SQL query.\"\"\"\n",
        "    \n",
        "    # Open the SQLite connection here\n",
        "    conn = sqlite3.connect('main.db')\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    query = state[\"query\"]\n",
        "    \n",
        "    # Execute the query and fetch the result\n",
        "    cursor.execute(query)\n",
        "    result = cursor.fetchall()  # Assuming you want to fetch all rows\n",
        "    \n",
        "    # Close the connection\n",
        "    conn.close()\n",
        "    \n",
        "    # Return the result (you can format or process it as needed)\n",
        "    return {\"result\": result}\n",
        "\n",
        "\n",
        "# Function to generate answer based on SQL query results\n",
        "def generate_answer(state: SqlState):\n",
        "    \"\"\"Answer question using retrieved information as context.\"\"\"\n",
        "    prompt = (\n",
        "        \"Given the following user question, corresponding SQL query, \"\n",
        "        \"and SQL result, answer the user question.\\n\\n\"\n",
        "        f'Question: {state[\"question\"]}\\n'\n",
        "        f'SQL Query: {state[\"query\"]}\\n'\n",
        "        f'SQL Result: {state[\"result\"]}'\n",
        "    )\n",
        "    response = gpt.invoke(prompt)\n",
        "    return {\"answer\": response.content}\n",
        "\n",
        "\n",
        "# SQL SubGraph!\n",
        "def sql_subgraph(state: SqlState):\n",
        "    \"\"\"Handle SQL-based questions using the sql_builder.\"\"\"\n",
        "    \n",
        "    # Just invoke the sql_builder with the current state\n",
        "    result = sql.invoke({\"question\": state[\"question\"]}, config=config)\n",
        "    \n",
        "    # Return the answer generated by the workflow\n",
        "    return {\"answer\": result[\"answer\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3. SQL Builder to finish the sequence and test the builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Michael Jordan wrote a total of 113 papers.\n"
          ]
        }
      ],
      "source": [
        "# Create the state graph to define the workflow\n",
        "sql_builder = StateGraph(SqlState).add_sequence(\n",
        "  [write_query, execute_query, generate_answer]\n",
        ")\n",
        "\n",
        "\n",
        "# Add the starting edge to the workflow\n",
        "sql_builder.add_edge(START, \"write_query\")\n",
        "\n",
        "\n",
        "# Compile the workflow\n",
        "sql = sql_builder.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "# Example query\n",
        "result = sql.invoke({\"question\": \"How many papers did Michael Jordan write?\"}, config=config)\n",
        "\n",
        "\n",
        "# Print the result\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Determining if a question should be answered using RAG or our SQL database of inferential/exploratory statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1. Question Classifier and Router\n",
        "Prompt engineering needs to be further perfected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Question classifier\n",
        "def classify_question(state: ParentGraph):\n",
        "    \"\"\"Classify the type of question: SQL or RAG.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    classification_prompt = f\"\"\"\n",
        "    You are a classifier. Determine the appropriate category for the following question:\n",
        "    - \"SQL\" if the question is related to SQL queries, databases, retrieving structured data, or performing data analysis using SQL or relational database concepts. Mainly if the user asks about what data you are mainly working from, and the statistics from this data such as \"who wrote the most papers?\" or \"how many papers did <full_name> write?\".\n",
        "    - \"RAG\" if the question involves retrieving information from the research papers, combining data from multiple sources, or problem-solving using Retrieval-Augmented Generation (RAG) techniques.\n",
        "\n",
        "    Question: {question}\n",
        "    Answer with only \"SQL\" or \"RAG\".\n",
        "    \"\"\"\n",
        "\n",
        "    question_type = gpt.invoke(classification_prompt).content.strip()\n",
        "\n",
        "    return {\"question_type\": question_type, \"question\": question}\n",
        "\n",
        "\n",
        "# Router to pick RAG/SQL to answer\n",
        "def route_based_on_question_type(state: ParentGraph):\n",
        "    if state[\"question_type\"] == \"SQL\":\n",
        "        return \"sql_subgraph\"\n",
        "    elif state[\"question_type\"] == \"RAG\":\n",
        "        return \"rag_subgraph\"\n",
        "    else:\n",
        "        return END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Finalizing \"nodes\" to the graph functions with a conditional \"edge\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x2a38d2b5220>"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calling parent graph function to determine state/class of query/answer\n",
        "parent_graph = StateGraph(ParentGraph)\n",
        "\n",
        "\n",
        "# Add nodes to the graph\n",
        "parent_graph.add_node(\"classify_question\", classify_question)\n",
        "parent_graph.add_node(\"sql_subgraph\", sql_subgraph)\n",
        "parent_graph.add_node(\"rag_subgraph\", rag_subgraph)\n",
        "\n",
        "\n",
        "# Conditional \"edge\" for routing based on classification\n",
        "parent_graph.add_conditional_edges(\"classify_question\", route_based_on_question_type)\n",
        "\n",
        "\n",
        "# Starting \"edge\"\n",
        "parent_graph.add_edge(START, \"classify_question\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Compiling all graph functions to generate results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The SQL query returned a result of 113 papers written by Michael Jordan.\n"
          ]
        }
      ],
      "source": [
        "# Memory saver to save states for persistence, checkpointing at every super-step\n",
        "memory = MemorySaver()\n",
        "parent = parent_graph.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "# Example query\n",
        "query = \"How many papers did Michael Jordan write??\"\n",
        "result = parent.invoke({\"question\": query}, config= config)\n",
        "\n",
        "\n",
        "# Moment of truth.....\n",
        "answer = result[\"answer\"]\n",
        "print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Gradio Interface!\n",
        "spnc: i have no idea what this part does so i can't annotate this, astrid and dr chat giga carried this...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Store conversation history for display in the UI\n",
        "conversation_history = []\n",
        "\n",
        "def process_question(user_input):\n",
        "    global memory  # Keep track of memory state, but idk how to stop it from persisting and answering previous questions yet\n",
        "    \n",
        "    # Add the user's question to the history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Combine all conversation history to provide context\n",
        "    conversation_context = \"\\n\".join(conversation_history)\n",
        "\n",
        "    try:\n",
        "        # Send the conversation context along with the new question and use memory for persistence\n",
        "        result = parent.invoke({\"question\": conversation_context}, config=config)\n",
        "        answer_text = result[\"answer\"]\n",
        "\n",
        "        # Add the model's answer to the history\n",
        "        conversation_history.append(f\"Model: {answer_text}\")\n",
        "        \n",
        "        # Return the answer\n",
        "        return answer_text\n",
        "    except Exception as e:\n",
        "        return f\"Error processing your question: {str(e)}\"\n",
        "    \n",
        "\n",
        "banner_url = 'https://img.freepik.com/premium-vector/classroom-vector-illustration-back-school-after-holiday-cartoon-classroom_1020331-7709.jpg?w=1060'\n",
        "\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks() as iface:\n",
        "    # Add the banner image\n",
        "    gr.Markdown(f\"\"\"\n",
        "    <div style=\"text-align: center; margin-bottom: 20px;\">\n",
        "        <img src=\"{banner_url}\" alt=\"Professor John 'Master Splinter' Yeow\" style=\"max-width: 100%; height: auto;\">\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Title and description\n",
        "    gr.Markdown(\"\"\"\n",
        "    ## Chat with Professor John 'Master Splinter' Yeow's Q&A System\n",
        "    You can ask questions, and the system will remember previous conversation context.\n",
        "    \"\"\")\n",
        "\n",
        "    # User input for the question\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(\n",
        "            label=\"Ask Your Question\", lines=2, placeholder=\"Enter your question here...\"\n",
        "        )\n",
        "    \n",
        "    # Output for the answer\n",
        "    with gr.Row():\n",
        "        answer_output = gr.Textbox(label=\"Answer\", lines=8)\n",
        "\n",
        "    # Add the submit button\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    \n",
        "    # When the submit button is clicked, process the question and display the answer\n",
        "    submit_btn.click(\n",
        "        fn=process_question,\n",
        "        inputs=question_input,\n",
        "        outputs=answer_output\n",
        "    )\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    iface.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing and debugging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmvHemRVqXlE",
        "outputId": "8c56d35c-3e81-4138-fc2b-ad0bed6c8eb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backpropagation is a learning algorithm for continually running fully recurrent neural networks. This information is from the paper \"A learning algorithm for continually running fully recurrent neural networks\" by R. J. Williams and D. Zipser in Neural Computation, 1989.\n",
            "Retrieved documents: [Document(id='a949ef01-b618-4d37-9c75-519125bece58', metadata={}, page_content='in Proceedings of the 16th conference on Uncertainty in artiﬁcial intelligence, 2000, pp. 626–633.\\n\\n[15] J. S. Yedidia, W. T. Freeman, Y. Weiss et al., “Generalized belief propagation,” in Advances in Neural\\n\\nInformation Processing Systems, vol. 13, 2000, pp. 689–695.\\n\\n[16] W. R. Gilks, S. Richardson, and D. Spiegelhalter, Markov chain Monte Carlo in practice. CRC Press,\\n\\n1995.\\n\\n[17] S. Chib and E. Greenberg, “Understanding the Metropolis-Hastings algorithm,” The American Statisti-\\n\\ncian, vol. 49, no. 4, pp. 327–335, 1995.\\n\\n[18] V. Kolmogorov and R. Zabin, “What energy functions can be minimized via graph cuts?” IEEE Transac-\\n\\ntions on Pattern Analysis and Machine Intelligence, vol. 26, no. 2, pp. 147–159, 2004.\\n\\n[19] R. G. Andrzejak, D. Chicharro, C. E. Elger, and F. Mormann, “Seizure prediction: Any better than\\n\\nchance?” Clinical Neurophysiology, vol. 120, no. 8, pp. 1465–1478, 2009.'), Document(id='ac4a12f2-adfc-4806-87ac-d3fba7436c5e', metadata={}, page_content='on this potential connection seems warranted.\\nDifﬁculty of optimization. While our experimental results are encouraging, we observe that when\\ntraining stops, the attribution regularization term remains signiﬁcant (typically around tens to hun-\\ndreds), which indicates ineffective optimization for the objectives. To this end, a main problem is\\nnetwork depth, where as depth increases, we get very unstable trajectories of gradient descent, which\\nseems to be related to the use of second order information during robust attribution optimization (due\\nto summation approximation, we have ﬁrst order terms in the training objectives). Therefore, it is\\nnatural to further study better optimization tchniques or better architectures for robust attribution\\ntraining.'), Document(id='453a13e7-a6d0-46bc-9c87-aaab544ac476', metadata={}, page_content='prevent neural networks from overﬁtting. JMLR, 2014.\\n\\n[51] X. Sun, J. Shang, S. Liang, and Y. Wei. Compositional human pose regression. In Proc. ICCV, 2017.\\n\\n[52] B. Tekin, P. Márquez-Neila, M. Salzmann, and P. Fua. Learning to fuse 2D and 3D image cues for\\n\\nmonocular body pose estimation. In Proc. ICCV, 2017.\\n\\n[53] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri. A closer look at spatiotemporal\\n\\nconvolutions for action recognition. In Proc. CVPR, 2018.\\n\\n[54] M. Vetterli, J. Kovaˇcevi´c, and V. K. Goyal. Foundations of signal processing. Cambridge University Press,\\n\\n2014.\\n\\n[55] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. Phoneme recognition using time-delay\\n\\nneural networks. Backpropagation: Theory, Architectures and Applications, 1995.\\n\\n[56] R. J. Williams and D. Zipser. A learning algorithm for continually running fully recurrent neural networks.\\n\\nNeural computation, 1989.'), Document(id='9edacada-e277-4111-a61a-4ab28cbfa37b', metadata={}, page_content='Understanding  this  relationship  is  a  main goal of the  present  work,  and enables  us \\nto prove a positive result about the power of queries.  Our work is derived within the \\nquery filtering paradigm, rather than the  constructive paradigm.  In this  paradigm, \\nproposed  by  [CAL90],  the  learner  is  given  access  to  a  stream  of inputs  drawn  at \\nrandom from  a  distribution.  The learner  sees  every  input,  but  chooses  whether  or \\nnot  to query  the teacher  for  the  label.  This paradigm is  realistic in  contexts  where \\nit  is  cheap  to  get  unlabeled  examples,  but  expensive  to  label  them.  It avoids  the \\nproblems  with  the  constructive  paradigm described  in  [ER90]  because  it gives  the \\nlearner free  access  to the input distribution.')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# retriever = vector_store.as_retriever()\n",
        "# # Initialize the RetrievalQA chain\n",
        "# qa_chain = RetrievalQA.from_chain_type(\n",
        "#     llm=gpt, \n",
        "#     chain_type=\"stuff\",  # 'stuff' means the full document is fed to the model\n",
        "#     retriever=retriever\n",
        "# )\n",
        "\n",
        "# # Example query\n",
        "# query = \"What is backpropogation? Quote the paper you are retrieving from.\"\n",
        "\n",
        "\n",
        "# # Use the QA chain to answer the query based on retrieved documents\n",
        "# response = qa_chain.run(query)\n",
        "\n",
        "# print(response)\n",
        "\n",
        "# # Retrieve documents manually\n",
        "# results = retriever.get_relevant_documents(\"What is backpropogation?\")\n",
        "# print(f\"Retrieved documents: {results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Backpropagation is a common method used in training artificial neural networks. It involves calculating the gradient of the loss function with respect to the weights of the network, which allows for adjusting the weights in a way that minimizes the error in the network's output.\n"
          ]
        }
      ],
      "source": [
        "# query = \"What is backpropagation?\"\n",
        "# result = qa_chain.run(query)\n",
        "# print(result)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
